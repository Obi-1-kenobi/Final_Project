{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "# Amazon stock market\n",
    "ticker = \"APPL\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "76/76 [==============================] - 20s 226ms/step - loss: 0.0017 - mean_absolute_error: 0.0264 - val_loss: 1.5024e-04 - val_mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00015, saving model to results/2021-07-07_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 16s 214ms/step - loss: 3.9401e-04 - mean_absolute_error: 0.0133 - val_loss: 1.1621e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00015 to 0.00012, saving model to results/2021-07-07_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 3/10\n",
      "76/76 [==============================] - 15s 202ms/step - loss: 4.8088e-04 - mean_absolute_error: 0.0144 - val_loss: 2.8574e-04 - val_mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00012\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 16s 205ms/step - loss: 4.6184e-04 - mean_absolute_error: 0.0142 - val_loss: 1.1955e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00012\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 15s 194ms/step - loss: 4.1675e-04 - mean_absolute_error: 0.0138 - val_loss: 1.2529e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00012\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 15s 196ms/step - loss: 4.2919e-04 - mean_absolute_error: 0.0143 - val_loss: 1.8697e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00012\n",
      "Epoch 7/10\n",
      "76/76 [==============================] - 15s 199ms/step - loss: 4.5025e-04 - mean_absolute_error: 0.0149 - val_loss: 1.2825e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00012\n",
      "Epoch 8/10\n",
      "76/76 [==============================] - 15s 194ms/step - loss: 3.6316e-04 - mean_absolute_error: 0.0132 - val_loss: 1.2585e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00012\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 15s 199ms/step - loss: 4.1609e-04 - mean_absolute_error: 0.0139 - val_loss: 1.3909e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00012\n",
      "Epoch 10/10\n",
      "76/76 [==============================] - 15s 199ms/step - loss: 4.1606e-04 - mean_absolute_error: 0.0138 - val_loss: 1.4702e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00012\n"
     ]
    }
   ],
   "source": [
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 3490.08$\n",
      "huber_loss loss: 0.00011621248268056661\n",
      "Mean Absolute Error: 28.699314225178828\n",
      "Accuracy score: 0.5195344970906068\n",
      "Total buy profit: 10785.618386268616\n",
      "Total sell profit: -543.9614720344543\n",
      "Total profit: 10241.656914234161\n",
      "Profit per trade: 8.51343051889789\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5lElEQVR4nO3dd3gVVfrA8e+bQgpBeg+9CKEFCAiKgiJFdxFRWdFV7GBBZV39ibrr6tpdu64FewWxoOgCFoogohRBAek9EJIQAklIAsnN+/tjJuEmpAG5uTfh/TzPfTJz5szc9xBy33vOzJwRVcUYY4wpTZC/AzDGGBP4LFkYY4wpkyULY4wxZbJkYYwxpkyWLIwxxpQpxN8B+EqDBg20devW/g7DGGOqlOXLl+9V1YZFy6ttsmjdujXLli3zdxjGGFOliMj24sptGMoYY0yZLFkYY4wpkyULY4wxZaq25yyKk5OTQ3x8PNnZ2f4OxZRTeHg40dHRhIaG+jsUY05qPksWIhIOLADC3Pf5VFX/JSIPADcAyW7Ve1V1prvPPcB1gAe4TVW/cct7A+8AEcBM4HY9jkmt4uPjqVWrFq1bt0ZETqR5phKoKikpKcTHx9OmTRt/h2PMSc2Xw1CHgHNUtQcQCwwXkX7utmdVNdZ95SeKGGAM0AUYDrwsIsFu/VeAcUAH9zX8eALKzs6mfv36liiqCBGhfv361hM0JgD4LFmoI8NdDXVfpfUGRgJTVfWQqm4FNgF9RaQpcIqqLnZ7E+8BFx5vXJYoqhb7fRkTGHx6gltEgkVkJZAEfKeqv7ibJojI7yLylojUdcuaAzu9do93y5q7y0XLjTHGePnqK9he7F0SJ86nyUJVPaoaC0Tj9BK64gwptcMZmkoAnnarF/cVUkspP4qIjBORZSKyLDk5ubgqAWH69OmICOvWrSuz7nPPPUdmZuZxv9c777zDhAkTii1v2LAhsbGxxMTE8Prrrxe7/4wZM3j88ceP+/2NMZXnggugY0ffHLtSLp1V1f3AfGC4qia6SSQPeB3o61aLB1p47RYN7HbLo4spL+59JqtqnKrGNWx41N3qAWPKlCkMGDCAqVOnlln3RJNFaS699FJWrlzJ/Pnzuffee0lMTCy0PTc3lwsuuIBJkyb55P2NMRXjk0/gqaec5cOHffMePksWItJQROq4yxHAucA69xxEvlHAand5BjBGRMJEpA3OiewlqpoApItIP3EGsMcCX/oqbl/LyMhg0aJFvPnmm4WShcfj4c4776Rbt250796dF198kRdeeIHdu3dz9tlnc/bZZwMQFRVVsM+nn37K1VdfDcBXX33FaaedRs+ePTn33HOP+uAvTaNGjWjXrh3bt2/n6quv5o477uDss8/m7rvvLtQzSUxMZNSoUfTo0YMePXrw008/AfDBBx/Qt29fYmNjGT9+PB6P50T/mYwxx+Avf4G77jqynpdX8e/hy/ssmgLvulc0BQHTVPVrEXlfRGJxhpK2AeMBVHWNiEwD/gBygVtUNf9T5yaOXDo7y32dkIkTYeXKEz1KYbGx8Nxzpdf54osvGD58OB07dqRevXr8+uuv9OrVi8mTJ7N161ZWrFhBSEgI+/bto169ejzzzDPMmzePBg0alHrcAQMG8PPPPyMivPHGGzz55JM8/fTTpe6Tb8uWLWzZsoX27dsDsGHDBr7//nuCg4N55513CurddtttDBw4kOnTp+PxeMjIyGDt2rV8/PHHLFq0iNDQUG6++WY+/PBDxo4dW673NsaU048/wumnQ1Dx3/Gv4w1yCeFdriYtDerUqdi391myUNXfgZ7FlF9Zyj6PAI8UU74M6FqhAfrJlClTmDhxIgBjxoxhypQp9OrVi++//54bb7yRkBDnV1KvXr1jOm58fDyXXnopCQkJHD58uFz3JXz88cf8+OOPhIWF8dprrxW85+jRowkODj6q/ty5c3nvvfcACA4Opnbt2rz//vssX76cPn36AJCVlUWjRo2OKXZjTBm+/x6GDIH//AfuvLPYKm9wAwBnvnF1hScKOMnu4PZWVg/AF1JSUpg7dy6rV69GRPB4PIgITz75JKparstEvet4339w6623cscdd3DBBRcwf/58HnjggTKPdemll/LSSy8dVV6zZs3yNQjnxrmrrrqKxx57rNz7GGOO0ZYtzs/168usGhHhmxBsbqhK9OmnnzJ27Fi2b9/Otm3b2LlzJ23atOHHH39k6NChvPrqq+Tm5gKwb98+AGrVqkV6enrBMRo3bszatWvJy8tj+vTpBeUHDhygeXPniuJ3333XJ/EPHjyYV155BXDOsaSlpTF48GA+/fRTkpKSCuLe7qtr94w5WeXkOD+LmfYmIwOCOHKe0JJFNTBlyhRGjRpVqOziiy/mo48+4vrrr6dly5Z0796dHj168NFHHwEwbtw4zjvvvIIT3I8//jh//vOfOeecc2ja9Mi1Ag888ACjR4/mzDPPLPP8xvF6/vnnmTdvHt26daN3796sWbOGmJgYHn74YYYOHUr37t0ZMmQICQkJPnl/Y05a7pdIQo4MBqWmwoEDsHcv9GVJQXlkpG9CkOOYYqlKiIuL06IPP1q7di2dO3f2U0TmeNnvzZz0nnwS7r4b7rgD3AtXRJzE8PPP8Ej3qUzlMgAWzjnMmecc/8SbIrJcVeOKllvPwhhjAt3+/c7PZ54BEfJPXWZmwoH9Sk0OFlStlXfAJyFYsjDGmECXnyyKuJY3GXBWEI05cl9VzZzi654oSxbGGBPAvvoKpr2WWqhMcO66e5PrAWjPpoJtEYf2+yQOSxbGGBPAHngATskrnCxOIY32bCxYb0RSwXLk4f0+icOShTHGBLDwcKjD/kJlLdjJRo7MGOg9DFX7vlvABxcuWbIwxpgA9tNPUJfCPYtWFL6XyTtZBG/aADt3UtEsWVSy4OBgYmNj6dq1K6NHjz6hGWWvvvpqPv30UwCuv/56/vjjjxLrzp8/v2Div2PRunVr9u7dW2x5t27d6NGjB0OHDmXPnj3F7n/++eezv4STc8aY0h10L3Iq2rNow9ZC641JJJU6BOFBN22Gli0rPBZLFpUsIiKClStXsnr1amrUqMGrr75aaPvxztj6xhtvEBMTU+L2400WpZk3bx6//fYbcXFxPProo4W2qSp5eXnMnDmTOr6YqMaYk4AzybQe1bN4kdsKrYdxmEOEMeDMIKRdW5/EYsnCj84880w2bdrE/PnzOfvss7n88svp1q0bHo+Hu+66iz59+tC9e3dee+01wPkAnjBhAjExMfzpT38qmGIDYNCgQeTfhDh79mx69epFjx49GDx4MNu2bePVV1/l2WefJTY2loULF5KcnMzFF19Mnz596NOnD4sWLQKc+auGDh1Kz549GT9+POW5afOss85i06ZNbNu2jc6dO3PzzTfTq1cvdu7cWahn8t577xXcoX7llc58kiXFYYxxRJJJDXLKrJdNOK1b+y6Ok3YiQb/NUe7Kzc1l1qxZDB8+HIAlS5awevVq2rRpw+TJk6lduzZLly7l0KFDnHHGGQwdOpQVK1awfv16Vq1aRWJiIjExMVx77bWFjpucnMwNN9zAggULaNOmTcFU5zfeeCNRUVHc6c5Yefnll/O3v/2NAQMGsGPHDoYNG8batWt58MEHGTBgAPfffz//+9//mDx5cplt+frrr+nWrRsA69ev5+233+bll18uVGfNmjU88sgjLFq0iAYNGhTMfXX77bcXG4cxxlF0CCrfw9zHNP7CXM6hASlkE+6L89oFTt5k4SdZWVnExsYCTs/iuuuu46effqJv374F04p/++23/P777wXnIw4cOMDGjRtZsGABl112GcHBwTRr1oxzzjnnqOP//PPPnHXWWQXHKmmq8++//77QOY60tDTS09NZsGABn3/+OQB/+tOfqFu3brH7A5x99tkEBwfTvXt3Hn74Yfbv30+rVq3o16/fUXXnzp3LJZdcUjBvVX5cJcVRq1atEt/XmJNJ0SGoD/gr6dTiv9zCHpqylwY0IIVDhHHFFb6L4+RNFv6Yo5wj5yyK8p4WXFV58cUXGTZsWKE6M2fOLHMa8/JOdZ6Xl8fixYuJKGaKyvLsDxz1UKb9+/eXOL15SXGVFocxJ7saNaCOe9/EIk7nDH5iMf15mVsK6iTQlE6sp8c5DegxrIQDVQA7ZxGAhg0bxiuvvEKOOy3xhg0bOHjwIGeddRZTp07F4/GQkJDAvHnzjtq3f//+/PDDD2zd6lwtUdJU50OHDi30LIv8BHbWWWfx4YcfAjBr1ixSUwt/qzlegwcPZtq0aaSkpBSKq6Q4jDHO41Fvu2AbAPfyKOsuupe3uQaAkSOdke8c3EkDixlpqEiWLALQ9ddfT0xMDL169aJr166MHz+e3NxcRo0aRYcOHejWrRs33XQTAwcOPGrfhg0bMnnyZC666CJ69OjBpZdeCsCIESOYPn16wQnuF154gWXLltG9e3diYmIKrsr617/+xYIFC+jVqxfffvstLSvoErwuXbpw3333MXDgQHr06MEdd9wBUGIcxpzsVJ2Zyf8yw7kYZDfNSL3zEabPjuTRR+GLL6BmTWiL+2CkYj4PKpJNUW4Cnv3ezMnI43EeX6E4w7cRZLJwaQRxXpOHn3469Fr8Ei9xqzMFbQUM59oU5cYYU4V4PEeegPcvHiCbiKMelBcXB/9lAmvX5PnuEXkunyULEQkXkSUi8puIrBGRB93yeiLynYhsdH/W9drnHhHZJCLrRWSYV3lvEVnlbntBynsG1hhjqqjcXAgnG4BMnMffFU0W//kPLFwInWN8/5Hoy57FIeAcVe0BxALDRaQfMAmYo6odgDnuOiISA4wBugDDgZdFJNg91ivAOKCD+xp+vEFV12G36sp+X+Zk5fE4N+QBHBKn11A0WYSFwYABlROPz5KFOjLc1VD3pcBI4F23/F3gQnd5JDBVVQ+p6lZgE9BXRJoCp6jqYnU+Od7z2ueYhIeHk5KSYh9AVYSqkpKSQnh4uL9DMabS5eYeSRY5ocX3LCqTT++zcHsGy4H2wH9V9RcRaayqCQCqmiAijdzqzYGfvXaPd8ty3OWi5cW93zicHkixV/FER0cTHx9PcnLyCbXLVJ7w8HCio6P9HYYxlS43FyLIcpZrRMLhapwsVNUDxIpIHWC6iHQtpXpxg25aSnlx7zcZmAzO1VBFt4eGhhbc2WyMMYHMexgqLywCMpwyf6mUq6FUdT8wH+dcQ6I7tIT7M382vHighddu0cButzy6mHJjjKm2vIehbr07ktNPh6ZN/RePL6+Gauj2KBCRCOBcYB0wA7jKrXYV8KW7PAMYIyJhItIG50T2EnfIKl1E+rlXQY312scYY6olj+fIMFTs6ZEsWlR9h6GaAu+65y2CgGmq+rWILAamich1wA5gNICqrhGRacAfQC5wizuMBXAT8A4QAcxyX8YYU2159yyIjPRvMPgwWajq70DPYspTgMEl7PMI8Egx5cuA0s53GGNMtVIoWQTARJt2B7cxxgSgw4cDq2dhycIYYwJQVtaRcxaWLIwxxhQrM9OGoYwxxpTBkoUxxpgy5Q9D5YWFQ5D/P6r9H4Exxpij5PcsNML/5yvAkoUxxgSkrCx3GCrc/0NQYMnCGGMCUmamezVUTetZGGOMKUF+z0IsWRhjjClJ/jkLibRhKGOMMSXIyoKaQVlIANyQB5YsjDEmIGVmQpRkBsTd22DJwhhjAlJWFkRasjDGGFMa55xFVkDcvQ2WLIwxJiBlZkKkHrSehTHGmJItXaLU8qRC3br+DgWwZGGMMQFHFQ4mZRCCx5KFMcaY4uXkQD32OSv16vk3GJclC2OMCTA5OdCIJGelQQP/BuPyWbIQkRYiMk9E1orIGhG53S1/QER2ichK93W+1z73iMgmEVkvIsO8ynuLyCp32wsiIr6K2xhj/O3wYYgm3lmJjvZvMK4QHx47F/i7qv4qIrWA5SLynbvtWVV9yruyiMQAY4AuQDPgexHpqKoe4BVgHPAzMBMYDszyYezGGOM3OTnQnF3OSoAkC5/1LFQ1QVV/dZfTgbVA81J2GQlMVdVDqroV2AT0FZGmwCmqulhVFXgPuNBXcRtjjL/l9yw8ITWq/zCUNxFpDfQEfnGLJojI7yLylojkn+pvDuz02i3eLWvuLhctL+59xonIMhFZlpycXJFNMMaYSnP4MLRmG5l1mgXEU/KgEpKFiEQBnwETVTUNZ0ipHRALJABP51ctZnctpfzoQtXJqhqnqnENGzY80dCNMcYvfvkF+rCULafE+juUAj5NFiISipMoPlTVzwFUNVFVPaqaB7wO9HWrxwMtvHaPBna75dHFlBtjTLW0bRucQhqH6zX1dygFfHk1lABvAmtV9Rmvcu/WjwJWu8szgDEiEiYibYAOwBJVTQDSRaSfe8yxwJe+itsYY/wtdXsaDdlLpz61/B1KAV9eDXUGcCWwSkRWumX3ApeJSCzOUNI2YDyAqq4RkWnAHzhXUt3iXgkFcBPwDhCBcxWUXQlljKmWliyB21/rDEBEszr+DcaLOBcYVT9xcXG6bNkyf4dhjDHHRAQ0/1Tt99/D4MGV/P6yXFXjipYHxml2Y4wxBZbTi4w6zSs9UZTGkoUxxgSYEHI50KGPv8MoxJKFMcYEmHCyqXFKuL/DKMSShTHGBJgwDhFW25KFMcaYUoSTTXjtMH+HUYglC2OMCTBRwTYMZYwxphTBwRAmhyA8sJKFL2/KM8YYcwxUweNRQsmGMBuGMsYYU4y8PKjBYWclwHoWliyMMSZAeDzOlVCA9SyMMcYUz+NxroQCrGdhjDGmeLm5gZss7AS3McYEgDZt4MwzbRjKGGNMKQ5uS6L/+zdRl1SnwHoWxhhjvOXlwX08wk28SjDuY3wCLFlYz8IYY/wsLQ0yiQSgFdudQhuGMsYY4y01FVKoD0A08U6h9SyMMcZ4S0098nS85uxyCi1ZGGOM8ZaaCpFkAlCHA07hyTIMJSItRGSeiKwVkTUicrtbXk9EvhORje7Pul773CMim0RkvYgM8yrvLSKr3G0viIj4Km5jjDkmM2bAQw+Vv35W1lFFKSkQQZHyk6hnkQv8XVU7A/2AW0QkBpgEzFHVDsAcdx132xigCzAceFlEgt1jvQKMAzq4r+E+jNsYY8pv5Ei4/37Ytq3sul9/DZGRsHJloeLdu4/0LAqcLD0LVU1Q1V/d5XRgLdAcGAm861Z7F7jQXR4JTFXVQ6q6FdgE9BWRpsApqrpYVRV4z2sfY4zxK09klLMwe3ap9d56CxaMfsFZWbOm0LZdu4pJFidRz6KAiLQGegK/AI1VNQGchAI0cqs1B3Z67RbvljV3l4uWF/c+40RkmYgsS05OrtA2GGNMUcnJsD6zhbOyZEnJFfft47rroEP2Kmc9I6PQ5l27Tu5hKABEJAr4DJioqmmlVS2mTEspP7pQdbKqxqlqXMOGDY89WGOMOQbPPgv12AeAJiYWX+mLL6B+fYYzi6bsccqSkgpVKbZnERlZwdGeGJ8mCxEJxUkUH6rq525xoju0hPsz/18tHmjhtXs0sNstjy6m3Bhj/GrP9kM0YC8AmrCn+Eo//gjA20HXHSkrklh2x+fRh6UF654a4RASWBNs+PJqKAHeBNaq6jNem2YAV7nLVwFfepWPEZEwEWmDcyJ7iTtUlS4i/dxjjvXaxxhj/Kb98o8JwcNumh6VAADWroX3XndmkW2SlwDAvnrtCtVVhX47P6Gl1yh8XkRNH0d+7HzZszgDuBI4R0RWuq/zgceBISKyERjirqOqa4BpwB/AbOAWVXUnSeEm4A2ck96bgVk+jNsYY8olbOcmPATxEZcjyUnOJ7+XJ5+EhmmbC9ZTqMfesOaFksW+fdA2Z12h/bRmlG8DPw4+6+eo6o8Uf74BYHAJ+zwCPFJM+TKga8VFZ4wxJ65e9i720IRdNEdycpy76+rVK9h+8CC0ZUvB+l4akExjOib+VlB2880wzJ0Paizv8h5XIVFVtGchIh1FZI6IrHbXu4vIP3wbmjHGBK59Kco1eW/RnN3soYlTuKfweYsg9dCabeynNgCh5LArtzFs2AD33QfAN99AezaR1OlMug5yLswJbdKg8hpSTuUdhnoduAfIAVDV33FuoDPGmJPSr3P3Fywn0thdKHzewrNjF2EcZrZ7H3GjsAOkZro32z36KOzdy/nnQzs206hfO/7vloPOtjZtfB3+MStvsohU1aIXEedWdDDGGFMVqMLr/9wBwJJh/ygxWQRtc4aglhEHQF5oGPMz+x6p8OGHtPnja5qzG9q3hxEjYOJEePppn7fhWJU3WewVkXa49zeIyCVAgs+iMsaYALZwIRxc71y9FDpqxJFhKK9kkZUFUUnOye01dAFAwyP4WEeT9M2vTqWJE3nktxHOcrt2zhQfzz4L9etXTkOOQXmTxS3Aa0AnEdkFTMS5QskYY046eXnQEqdnkdusJanUJS84pNA5i/79nZPbHglmtXt9Tmq/8wHhqf/FHH3Qzp0rI/TjVq5koapbVPVcoCHQSVUHqOo2n0ZmjDEBKiPDSRae4FA89RuhBJFTq55zHazrt9+cZJFWtxXxtOBU1tH842e4+GL4zwuFJwlMbNgFunWr7GYck/JeDfWoiNRR1YOqmi4idUXkYV8HZ4wxgehg6mEm8QRERREc6nyMHgqOPGr68XZsJq91OwA2cCqhkaHcdVfhYwnKx/9YDUGB/Xih8kZ3nqruz19R1VTgfJ9EZIwxAa7BD58BEHwgteAzPj4lgpwDhed3assWIru2LVTWsaPzsxNrORXnZrwuXXwbb0Uob7IIFpGCfpOIRACBNdm6McZUkqitvwOQ9cgzZDuzeZBJJGkbj5yzuGDgARqQQnhM4WRR133c23o6sYFTAejRw/cxn6jy3sH9ATBHRN7GuSLqWo48k8IYY04amzbBtrlbqEd72k36GylfO+VxLIe1kLNgMT9+nsQV25wJBKV9O4YPh0suOfpY06bBTz9Bg8C7B+8o5UoWqvqkiKzCmaZDgIdU9RufRmaMMQFo/Hh4nC1sph0dgqBPn8Lbv356PaNmXHOkoG1bZpUwm92oUTB6tO9irUjlnhtKVWdhE/gZY05yc+c6J66bjHCyRNOmzmNRaeZsj99b5KFFbQsPQ4Fzn8ZXXwXcLOSlKvWchYj86P5MF5E0r1e6iJT2ICNjjKmW6pBKPVJpcdaRJBAeDkP4FoCdP+0oKE8Pqw+1ax91jAED4IknfB9rRSo1WajqAPdnLVU9xetVS1VPqZwQjTEmcDzQ5FVnoV27grKwMFiK09OIdp8C/Th388w1qys9Pl8p82ooEQnKn23WGGNOWhs2wIwZ3L7nXmf91FMLNoWFQQZR5CEFd3YvpQ9BzZr4I1KfKHPETFXzROQ3EWmpqjvKqm+MMdVSz56Qmcl+apNbtxENYo5M2REcDB5C2EFLLnQf5JlOLZo391ewFa+891k0Bda4z7SYkf/yZWDGGBNQMp0b7upwgFX9bii2ygwuKFjOIIorr6yUyCpFec/FP+jTKIwxJoB5PBDsXdCwYbH1bucFbuNFAGb+EEVoqO9jqyxlXQ0VLiITgdFAJ2CRqv6Q/6qMAI0xxt8mjM8ptN7/gqPvonvvvcLrdaID7znaJ6KsYah3gThgFXAeUO4ncojIWyKS5H1yXEQeEJFdIrLSfZ3vte0eEdkkIutFZJhXeW8RWeVue0FESnqutzHG+MQ3b+4stB4e0+6oOkcNOdWq5cOIKl9ZySJGVa9Q1deAS4Azj+HY74D7LMHCnlXVWPc1E0BEYnAe09rF3edlEcnv9b0CjAM6uK/ijmmMMT7Thq2FCxo1KnunqJOrZ1HQ91LVY3qMqqouAPaVWdExEpiqqodUdSuwCegrIk2BU1R1saoq8B5w4bHEYYwxJ0IVWrOtcGGdOiXWn4x78js8vMQ6VVFZyaKH913bQPcKuIN7goj87g5TufMv0hzw7ufFu2XN3eWi5cYYUyn27HF6Frnep7iDg0usfxOvwMGDUM1GzMu6gzu4yF3bISd4B/crQDsgFucZ3vnnQIr7V9VSyoslIuNEZJmILEtOTj6O8IwxprD1651ksZMW5aqfRzBERvo4qspXqY9mUtVEVfWoah7wOtDX3RQPhX4T0cButzy6mPKSjj9ZVeNUNa5hCZe2GWPMsfjgAydZbKUN43iNPc985O+Q/KJSk4V7DiLfKCD/SqkZwBgRCRORNjgnspeoagKQLiL93KugxoJ7e6QxxvjYvn3w5ptHksXrjKPWuMtKrD9nDvxQTW8q8NkEuSIyBRgENBCReOBfwCARicUZStoGjAdQ1TUiMg34A8gFblFVj3uom3CurIrAmSLdpkk3xlSK9HQIJ4um7GErbQCoWbPk+uecU0mB+YHPkoWqFpd+3yyl/iPAI8WULwO6VmBoxhhTLunp0IrtAAXJ4mRVqcNQxhhTlaSlHbnHwpKFMcaYYhVNFqUNQVV3liyMMaYEBw44w1DZhPHIG01YudLfEflPFXoCrDHGVK60NGhMIok05trrqtdNdsfKehbGGFOCtDRoRBL1O5VjLqhqzpKFMcaUYO5cp2dRs40lC0sWxhhTgi1bnJ6FNLZkYcnCGGNKkOdRmgQlQePG/g7F7yxZGGNMMVQhfUcqoXmHy/f8imrOkoUxxhQjMREuPDTVWWnf3r/BBABLFsYYU4x9XyzgeW4nu15TGDrU3+H4nSULY4wp4ttvYcZNMwHYNGNttXvq3fGwZGGMMUU89hj0ZQmbaUeLrrX9HU5AsGRhjDFFRNfJYBDz+YoR1LZcAViyMMaYo5zKeoJQdjTr7+9QAoYlC2OM8aIKm3/ZC8B9LzbxczSBw5KFMcZ4Wb8eMhP2A9CkUx2/xhJILFkYY4yXXbugDvudlTp1/BlKQLEpyo0xBuDFF2H/frJ6/tOSRTGsZ2GMOelNfjkXbrsN7r+frCyIJh5PzVoQEeHv0AKGz5KFiLwlIkkistqrrJ6IfCciG92fdb223SMim0RkvYgM8yrvLSKr3G0viMjJ/QQSY0zFeuIJxt0SWrCasy+dK3mfnPadwT5uCviyZ/EOMLxI2SRgjqp2AOa464hIDDAG6OLu87KIBLv7vAKMAzq4r6LHNMaY4zdpUqHVbtP+SR0OkHvaAD8FFJh8lixUdQGwr0jxSOBdd/ld4EKv8qmqekhVtwKbgL4i0hQ4RVUXq6oC73ntY4wxJ+Sp+1IByKAm/+AhALrNfR6AvItH+y2uQFTZJ7gbq2oCgKomiEj+vL/NgZ+96sW7ZTnuctHyYonIOJxeCC1btqzAsI0x1dHWRz8CYBDzWU4c0cRzI68xn4H0O6OPn6MLLIFygru4gUEtpbxYqjpZVeNUNa5hw4YVFpwxpnq6jCmsIJblxAFwE68iKIOD5hMWGVzG3ieXyk4Wie7QEu7PJLc8HmjhVS8a2O2WRxdTbowxJ2T14nT6soSFnAlAkPtpePrp8Oqrdm67qMpOFjOAq9zlq4AvvcrHiEiYiLTBOZG9xB2ySheRfu5VUGO99jHGmOO2945HqUEO5/1nMLt2wZo18Oij8OOPcMMN/o4u8PjsnIWITAEGAQ1EJB74F/A4ME1ErgN2AKMBVHWNiEwD/gBygVtU1eMe6iacK6sigFnuyxhjTkiNVcvYEXkqHe4cCUCzZnDPPX4OKoD5LFmo6mUlbBpcQv1HgEeKKV8GdK3A0IwxJ7ldM3/j9IPfs7LXtdilMOUTKCe4jTHGN557DlatOrK+dSuNR/TBQxDR19ttW+Vlc0MZY6qvnTvhb39zlnNywOOBtm0JAf7UeQv/u6mNX8OrSqxnYYypth67/EiPQn9dwWMX/gLAb3SnQR9LFMfCehbGmGopKwuSf1xXsL5n/loOzd6ChyBq/PQDL8T4MbgqyJKFMaZamvHcFu7mCfZRlzpygKZ3X8UDwIG2sXTuX8fP0VU9NgxljKl+Nmzg0nvb0ZgkXuRWvqs5qmBT5IRr/RhY1WU9C2NMtXJgn4fNcTfQlVAu5AtmcT5kwJksoF6nxnwxsaO/Q6ySLFkYY6qPX3+ldu/e9ALG8RqZA8+HH5xNCzmLuokUP+OcKZMlC2NMtZCX4yF15DXUd9dfSr2C4FoQ4vUpN22aX0KrFuychTGmWnj+htXUj/+dl7iFZffPoEadSIKDoUMHZ3tODpx7rn9jrMosWRhjqhZVDvc6zZkW9sknATh0CFa9twKA7Otvpdu9IwqqL1gAM2cW7mGYY2fJwhhTtSxZQo0VS5zlBx8EYPNmiNVfyQmryZ2vdSAs7Ej1Jk3gvPP8EGc1Y8nCGFOlfH/p6xwkkhmMcHoXqvz2G/RkBZntexx5MIWpUPavaoypMrZvh47bv+UrRvAtQ+HgQdizh8PZecSyEnr19HeI1ZYlC2NMlTF9Wg7N2UVWi1PZiHPmWjdu4okbNlGLDOjZy88RVl+WLIwxVcbCKfEEk8fCna3YRHsA0j/6ipGezwAIOet0f4ZXrVmyMMZUCQcOwL4V2wDo+5fWbKM1STQk9/t5jOArltCHiJ6d/BtkNWbJwhhTJcTHOyexAXr8pRN5BPMtQ6m3eRmns5ieD46yc9s+ZFceG2MCW14ePPUUyeua8Ax/J6d+Y/pd1IxRo2Df9HoAfMsQht57l58Drd78kodFZJuIrBKRlSKyzC2rJyLfichG92ddr/r3iMgmEVkvIsP8EbMxpvJ5PDDz4V/h7rsZ9PZVAIR4DiECnTsfqVd/1EC7687H/NlpO1tVY1U1zl2fBMxR1Q7AHHcdEYkBxgBdgOHAyyIS7I+AjTGV67vv4Mt/LS9UJosWARAaChtwZpDtffmplR7bySaQRvhGAu+6y+8CF3qVT1XVQ6q6FdgE9K388Iwxle2PP6APSwH4O09BZibEOI+4S0+Hl7mZT66dBRdf7M8wTwr+ShYKfCsiy0VknFvWWFUTANyfjdzy5sBOr33j3bKjiMg4EVkmIsuSk5N9FLoxprKsXw9xLGM2w/ikxd8hIqJgW1oaKEGknjbcuZPb+JS/BvnOUNXdItII+E5E1pVSt7j/BVpcRVWdDEwGiIuLK7aOMabq2L42k66s5itG0KJF4W2TJsGGDTBiRPH7morll2Shqrvdn0kiMh1nWClRRJqqaoKINAWS3OrxgPd/k2hgd6UGbIypXKqkzf6JsxfOIAQPu5rE8dRThau0awc//OCf8E5GlT4MJSI1RaRW/jIwFFgNzACucqtdBXzpLs8AxohImIi0AToASyo3amNMZfrugUWccv4A7uZJfm0+gle3DqN/f39HdXLzR8+iMTBdnDHGEOAjVZ0tIkuBaSJyHbADGA2gqmtEZBrwB5AL3KKqHj/EbYwpj0OHyLzvYSI6tEDGjyu7fhEpKTDn3z8yBDiVdcxbciqEV3yY5tiIavUc2o+Li9Nly5b5OwxjTirTn91G3zvOoHn+SPGWLdCmTfkPMHs22//xOoeWryK6uRIZv9E3gZoSichyr1saCgTSpbPGmCps+XLIvO8RmrObJ/g/p7BtW7RvX+cu7Hw5OcXu31QS4LzzaLX8czqykRq33lgJUZvysmRhjDlhmzfDS3Fv89esN9g+fBz38ijpRAEgS5fCOveCx9mzoUYNJ7McOFCwf3Y2xPAHAI9zNxNbfErI3X+v9HaYktn98caYE/bYYzCE70ihPi2/fIm8sGD6s5i/8SzX8Raer2eydkMNYi4Z4XxDjYuDhg1JW7SKsK8+JXl3Dm3c5DJm3o20HtTan80xxbBkYYw5IRkZsOyTrbzBFA6edg5SI5QPPwTVrlxxxRt0YQ19Jk2itj5PELlHdkxOZkWXvzIwZw4tgDMZiwYH03pAtN/aYkpmycIYc8z27YO29ffzM/34hmHM5mMAav5pEACXX+7Ue/tt4co577NRO9KCeP7Jv8kgijPDl7Iwuw/P5txRcMyreA+VEJsQMEDZb8UYU7bkZGjQwJkGNiSEWf/L49/cTyfW04n1ADze6Bkm3Vn4pPTXX0OHDh34d/w/GdFtO+M/u5Ok9AiaNIGLmyut2UYUGRygNnfwLHLLLf5onSkHu3TWGFMgLw8y1sVzSkgmdOzIr8uVuRe9yJ07bi+o8wUjiWUlrdnOrs7nsvns64kJXkedZ/5VbKcgJwd++QXOOOPoKZxSU2HFCvj3v2H82CwuuzoMe4KRf5V06awlC2OqscxMWLsWevcuu25uLsQ2TWTm3j60CElg2nXfEPXa0/yJmcXW3zr8RtrMeMGZK9xUG3afhTEnoSfGbaZBXCsyzh0JRb4YxsfDhx8onu/nsX5FJg/1+IQZe/vTkp1Ibi6XvjaYPzGThafdSU5KGh9PVcafsZq7rtlL5rodtJn1iiWKk4j1LIypZi68EOp/+SZPnPk/9i1cTUfcu6D/8Q946KGCetdcAynvzGAGIwvtv+SJeVx6dysm8BJX/nk/9ab8l5Aom2/jZGHDUMZUQ2lpEB6mHDgA/x6/i0kLzmPXvgj6spTttCSLCBZwFj1ZQR+WkfDBHJKi2hLZ5BRe7v8+z+pEAJLDmrO7Xje6T70XOetMEhNhxw7o08e/7TOVz5KFMdVEZia8/H/byPzvW4znNRqTxFQupTfL6cAmAA5KTZpKIi+9XZNx4yDq0F620BZFqMFhgsgjjMPOAS+7DD76yI8tMoGkpGRhl84aU0VkZkJMzW38g4e5g7cAUPfZYGPc+xw+u2QK+6O7csVfDpPUsybh4TB2LLz9dgN6XruCWSEjyJYIEut2JjwqmK6PX0ndIUd9LhhzFOtZGONvqrBnD9SuDZGRbNoET47bxEMTEml80RkAbNsGw7rtZmlGJ04hnV96jqfH1Hv5ZXcLvv4ojZuWX0fbawbBhAklvs3Bg1AjKJfQGgLBwZXSNFP1WM/CmACjChNuOMRf3jmfgZ65AMyPnUjiyt28zGeEzPOwbeTtbB5yI/ffeZD3s28iSg6S8cUcTrvgHAAGdoSBg2oDn5b5fjVrgv3Jm+Nl/3MCQHa2c417VJS/IzGVadYsaPTmowxkLovDB1EvezeDVj5HHsIvbcaQsv0gf/7yeVp/+TyD83d6+hmi3ERhTGWyZOED27c75wvvvhv274eXnjjI+KR/0/isTs71il5yk1OZcEckp37wD26NXUj4orkQGemfwA0JCfDYo0rzDXNpE7Sduft6cs0FKZx296CCOYtUndm169SBVauc4s6dSz9uRoYz0hQaCmt/z2F3fB7P3baFmcHvkDd4GP1mz+bDD2F9wnr+fPo++p/Rn6QkWD57DU2en0Ttmh6i3n8FWrXy+b+BMcWxcxY+cMUVsOLDNSxqN5a3Y56iyVeTuYypzkb3yWEZGXDemRl8vLIjzUgo2Hd65F+pOSiOyLtvY8BZds+kr+3fDz9+lQqffsJPPwczKulVwjlEN1YVqpca2YxVPa/i/kVDuYRPqUsqTRt6iE+uQR+WckqzWjSb/l+k75FrTZOSYNSFSrPFn3IRn9OPn8khlObsIog8IshGg4KQTz6Biy6q5JYbUzy7dLYy37tdKlO39KE9mwvK/gjqSru8DWyiPbXCDtPy0KZC+6xsOhxN2ENPVhaUfV3/KoLvuoPhd3VDgopMqhNADh+GDz6AtS/PY3itRWw9dRiba/dm6PAgBg1y5p7zeODnnyHPowwaqEjw8SfC7Gzn7uP69aFu3eOPe/p0eOXi75mml1CHIw/iOdykBclX/x/z1zXmIvmCL/YPova86ZzPrEL7J0kjGmkSeQgHqE1d9pM64kpuj3idmdPSOYe5BZPtpYU1IMiTw66GPakZqRxs0pZ2dfcR8tzT0K7d8TfCmApW5ZOFiAwHngeCgTdU9fHS6ldaslAtmIkT4Idvsqk9vB9dWMOdQc/yfN6t5JwzjIVXv8n0sZ/zPLezifZ0ZCN763XgwM33EjTmL9RuFMbiD7dwxv7/MXvaAc5d+yINSAEgIao9S3vfxNcp/Ti92Xbo3Zth5wXRdEC7o2dmqwS//Qbz58OSH7KI2PoHNdb9xsDsb7iETwnmyOMzE2jCWjrzAwPpyEYakMzp/ER2UCTf9H+Q7YeaoOkZHG7QjC2rMzmv3QZaNT1M6hl/JvfULqTsEw4fdhJD7kfTiMuYT1K9TuzZncepB5dzLt+zNaQjh3udxoFO/dh/6mnU7dKMFSuF00+HwYOdWL/7Dn6bt4/TMucR2rwRWz0tidy7gx/nHuJxJhHTOIV9z39AdKcoaN8+/0xwIZs2weZPfmWIfoNccQXSqCGEhcHMmeR1imH0+HrcMed8zuCnQvsdbtqK0HvvQm6+ySbIM1VClU4WIhIMbACGAPHAUuAyVf2jpH18nSxyc+GuMTsZ/+1FtMzdQuqwy0jcmEb9NT/Qih2kPfgsB2+YSANNJrRpAxDh88+hf2wWy1aHE5O6iHajexV7fiIvD2Z8qQzc/h7L31lFzG8fFRqqyjev803IlVfSfGQcrdqH8u23zrX4DRpAejoMGVL48IcPO+Pme/c628FZ3rbNGX9v1szJeQkJsHs3JKzYQ4udP0F4OMlh0STuOETEzg1Ep//BKKZzKusLksOhmnUJ+utlbBk9iRo/L6BJymp2Lk8icukPRGdvJiWiOad4UllVfxA5iamclre41H/fdKLYTx1yCSGPINqx5ag662Mu5MAfu4hlJTVwnuuch5BBFDU4zFZpywbtQEOSiWUlkWQV/2aTJ8MNN5QaT1ny8uDav2Rwy5dD6Bn0G7tHT6RZyxDn0aC1a5/QsY2pTFU9WfQHHlDVYe76PQCq+lhJ+xxvsnj6KWXHTuFgmofcjGxCDx8kTWuRk5ZFjdREmu5bTfOc7dTdv4XzD02nAXvZQEdas41gPOyr3ZY6f7uGyPvvrLBv/YnrUtn5+my6t07js6UtOG3Zf6m7aRl1c5IA54P1Z/qRQFN68Bt5BJFEI+qSyob6p/NHZBx7DkTQPW0hI/iKdGrRhq2kUpcdtCSUHBqRRAi5JNGIbMJpwU5as73EmLK69yX8wvOQbl2ha1do3RrCS5g/6OBB59u6qvNvkpMDc+aQV68BB7OCqBW/Flq3xtOqLYnJQYTMnEHe76uJzMsgVHIIIZfQLqfCLbc4xwgOdnpzDRsCkJeZzcFFKzm8aClZO5KpH5rG2i01iFq3jPqyj/Bm9anZqYXzRJ6sLCdD1qsHERHQogV06VIhvycAT64SnJfjPGfamCqoqieLS4Dhqnq9u34lcJqqTihSbxwwDqBly5a9t28v+cOuJCuiBlA/M56GJBGhJXwTBbKDIwn3ZHL4safZdekdLPsxm9jueXToUXlXMu38NZnsb34g97t5nLJ6EY32byCrdWfSIxsTsi8JT3YO9VM2EJaXXbBPUpu+hOdmkNkqBsJqEJm4jYgGkWTWbEROVi6SfoAwTybh7VsQ0qs7DBjgdFdSU50P15o1nQmDoqL8MgRmjPGtqp4sRgPDiiSLvqp6a0n7HPcw1EMPwfr10KiR8801Ksq57jEiAho3hrZtISbGKU9JccZ8AkX+N3dvOTmwcaPzTbxWLacHYIwxJajqd3DHAy281qOB3T55p3/+s/x1AylRQPHf9ENDneRmjDEnoKpcnrEU6CAibUSkBjAGmOHnmIwx5qRRJXoWqporIhOAb3AunX1LVdf4OSxjjDlpVIlkAaCqM6GEhwEbY4zxqaoyDGWMMcaPLFkYY4wpkyULY4wxZbJkYYwxpkyWLIwxxpSpStzBfTxEJBlKmdwocDUA9vo7iApibQlM1pbAFChtaaWqDYsWVttkUVWJyLLibrWviqwtgcnaEpgCvS02DGWMMaZMliyMMcaUyZJF4Jns7wAqkLUlMFlbAlNAt8XOWRhjjCmT9SyMMcaUyZKFMcaYMlmy8DERaSEi80RkrYisEZHb3fJ6IvKdiGx0f9b12uceEdkkIutFZJhXeW8RWeVue0Gkcp9rWpFt8do+Q0RWV2Y73PetyN/LZe7v5XcRmS0ilfpUrGNti4jUd+tniMhLXseJFJH/icg69ziPV2Y7KrIt7rYaIjJZRDa4bbo4wNsyRESWu/+XlovIOV7H8uvfPgCqai8fvoCmQC93uRawAYgBngQmueWTgCfc5RjgNyAMaANsBoLdbUuA/oAAs4Dzqmpb3O0XAR8Bq6vq7wVnmv8koIFb70nggQBvS01gAHAj8JLXcSKBs93lGsDCKvB/rNi2uNseBB52l4Pyf0cB3JaeQDN3uSuwy+tYfv3bV1VLFpX+Dw5fAkOA9UBTr/9U693le4B7vOp/4/4naQqs8yq/DHitKrbFXY4CfnT/eCo9WVTg7yUUSAZauX/IrwLjArktXvWuLvoBW2T788ANVbUtwE6gpr//bx1rW9xyAVJwvpwExN++DUNVIhFpjfPt4RegsaomALg/G7nVmuP8J88X75Y1d5eLlvvFCbYF4CHgaSCzMuItzYm0RVVzgJuAVTjPhY8B3qycyI9WzraU5zh1gBHAnIqPstwxtOY42+LGD/CQiPwqIp+ISGMfhluq42jLxcAKVT1EgPztW7KoJCISBXwGTFTVtNKqFlOmpZRXuhNti4jEAu1Vdbov4jsWFdCWUJxk0RNoBvyO0wupdMfQlrKOEwJMAV5Q1S0VFd8xxnCibQkBooFFqtoLWAw8VYEhltuxtkVEugBPAOPzi4qpVul/+5YsKoH7gfIZ8KGqfu4WJ4pIU3d7U5xxb3C+NbTw2j0a5xtrvLtctLxSVVBb+gO9RWQbzlBURxGZ7/voC6ugtsQCqOpmdcYIpgGn+z76wo6xLWWZDGxU1ecqPNByqKC2pOD0WvO/kHwC9PJBuKU61raISDROzGNVdbNbHBB/+5YsfMy9auFNYK2qPuO1aQZwlbt8Fc54Zn75GBEJE5E2QAdgidtdTReRfu4xx3rtUykqsC2vqGozVW2Nc3Jyg6oOqow25KuotgC7gBgRyZ+lcwiw1tfxezuOtpR2rIeB2sDECg6zXCqqLW7i/goY5BYNBv6o0GDLcKxtcYfO/odzbmxRfuVA+NvPD8Revj2pNQCny/g7sNJ9nQ/UxxkP3uj+rOe1z304V9usx+uqByAOWO1uewn3Dvyq2Bav7a3xz9VQFfl7uREnQfyO8wFVvwq0ZRuwD8jA+eYag/ONVd225B/n+qrYFre8FbDAPdYcoGUgtwX4B3DQq+5KoJG7za9/+6pq030YY4wpmw1DGWOMKZMlC2OMMWWyZGGMMaZMliyMMcaUyZKFMcaYMoX4OwBjqgMR8eBM+REK5ALvAs+pap5fAzOmgliyMKZiZKlqLICINMKZTbc28C9/BmVMRbFhKGMqmKomAeOACeJoLSIL3QntfhWR0wFE5H0RGZm/n4h8KCIXiEgXEVkiIivFeUZGB3+1xZh8dlOeMRVARDJUNapIWSrQCUgH8lQ12/3gn6KqcSIyEPibql4oIrVx7tjtADwL/KyqH4pIDZxngGRVaoOMKcKGoYzxnfzZQkOBl9zZdj1ARwBV/UFE/usOW10EfKaquSKyGLjPnVTuc1Xd6IfYjSnEhqGM8QERaYuTGJKAvwGJQA+cOX5qeFV9H/grcA3wNoCqfgRcAGQB33g/XtMYf7FkYUwFc2egfRXnyW2Kc6I7wb0y6kqcx7Hmewd3hldVXePu3xbYoqov4MxQ2r3SgjemBDYMZUzFiBCRlRy5dPZ9IH9a6peBz0RkNDAPZ2ZRAFQ1UUTWAl94HetS4AoRyQH2AP/2efTGlMFOcBvjRyISiXN/Ri9VPeDveIwpiQ1DGeMnInIusA540RKFCXTWszDGGFMm61kYY4wpkyULY4wxZbJkYYwxpkyWLIwxxpTJkoUxxpgy/T/8u8WuZtfpNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjclose_15</th>\n",
       "      <th>true_adjclose_15</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-08-06</th>\n",
       "      <td>2.208333</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1243200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-8.748855</td>\n",
       "      <td>2.317708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-07</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.260417</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>2.177083</td>\n",
       "      <td>2.177083</td>\n",
       "      <td>2034000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-8.921314</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.197917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-18</th>\n",
       "      <td>2.052083</td>\n",
       "      <td>2.052083</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>2.041667</td>\n",
       "      <td>2.041667</td>\n",
       "      <td>1784400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-9.788497</td>\n",
       "      <td>3.239583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.197916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-19</th>\n",
       "      <td>2.093750</td>\n",
       "      <td>2.208333</td>\n",
       "      <td>2.052083</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>1003200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-9.870554</td>\n",
       "      <td>3.302083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.135416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-21</th>\n",
       "      <td>2.135417</td>\n",
       "      <td>2.171875</td>\n",
       "      <td>2.072917</td>\n",
       "      <td>2.114583</td>\n",
       "      <td>2.114583</td>\n",
       "      <td>624000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-10.025806</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.572917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-05</th>\n",
       "      <td>2.583333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.458333</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1908000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-10.105702</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-08</th>\n",
       "      <td>2.531250</td>\n",
       "      <td>3.020833</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5648400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-9.969608</td>\n",
       "      <td>4.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-10</th>\n",
       "      <td>3.312500</td>\n",
       "      <td>3.328125</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>3.302083</td>\n",
       "      <td>3.302083</td>\n",
       "      <td>3866400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-9.441730</td>\n",
       "      <td>4.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-12</th>\n",
       "      <td>3.187500</td>\n",
       "      <td>3.697917</td>\n",
       "      <td>3.156250</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3333600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-9.037761</td>\n",
       "      <td>4.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-15</th>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.677083</td>\n",
       "      <td>3.052083</td>\n",
       "      <td>3.093750</td>\n",
       "      <td>3.093750</td>\n",
       "      <td>5583600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-8.836848</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-17</th>\n",
       "      <td>3.458333</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>2607600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-8.462343</td>\n",
       "      <td>4.005208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.598958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-22</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.677083</td>\n",
       "      <td>3.947917</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>16938000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-7.622609</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-29</th>\n",
       "      <td>4.145833</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>3.958333</td>\n",
       "      <td>4.041667</td>\n",
       "      <td>4.041667</td>\n",
       "      <td>2371200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-6.254580</td>\n",
       "      <td>3.822917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-06</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>3.942708</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2028000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-6.093350</td>\n",
       "      <td>4.270833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-17</th>\n",
       "      <td>3.614583</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.520833</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>2534400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-7.022874</td>\n",
       "      <td>4.479167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-21</th>\n",
       "      <td>3.958333</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>3.854167</td>\n",
       "      <td>4.427083</td>\n",
       "      <td>4.427083</td>\n",
       "      <td>12096000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-7.048798</td>\n",
       "      <td>3.947917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-31</th>\n",
       "      <td>5.364583</td>\n",
       "      <td>5.458333</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>5.083333</td>\n",
       "      <td>5.083333</td>\n",
       "      <td>5026800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-5.625153</td>\n",
       "      <td>4.489583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-11-05</th>\n",
       "      <td>4.979167</td>\n",
       "      <td>5.119792</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>3093600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-5.376828</td>\n",
       "      <td>4.260417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-11-07</th>\n",
       "      <td>4.416667</td>\n",
       "      <td>4.640625</td>\n",
       "      <td>4.385417</td>\n",
       "      <td>4.479167</td>\n",
       "      <td>4.479167</td>\n",
       "      <td>2626800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-5.358926</td>\n",
       "      <td>4.270833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-11-17</th>\n",
       "      <td>4.229167</td>\n",
       "      <td>4.541667</td>\n",
       "      <td>4.218750</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>7394400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-5.870422</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.312500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close  adjclose    volume ticker  \\\n",
       "1997-08-06  2.208333  2.312500  2.187500  2.250000  2.250000   1243200   AMZN   \n",
       "1997-08-07  2.250000  2.260417  2.125000  2.177083  2.177083   2034000   AMZN   \n",
       "1997-08-18  2.052083  2.052083  1.968750  2.041667  2.041667   1784400   AMZN   \n",
       "1997-08-19  2.093750  2.208333  2.052083  2.166667  2.166667   1003200   AMZN   \n",
       "1997-08-21  2.135417  2.171875  2.072917  2.114583  2.114583    624000   AMZN   \n",
       "1997-09-05  2.583333  2.666667  2.458333  2.500000  2.500000   1908000   AMZN   \n",
       "1997-09-08  2.531250  3.020833  2.500000  3.000000  3.000000   5648400   AMZN   \n",
       "1997-09-10  3.312500  3.328125  3.125000  3.302083  3.302083   3866400   AMZN   \n",
       "1997-09-12  3.187500  3.697917  3.156250  3.687500  3.687500   3333600   AMZN   \n",
       "1997-09-15  3.666667  3.677083  3.052083  3.093750  3.093750   5583600   AMZN   \n",
       "1997-09-17  3.458333  3.500000  3.333333  3.406250  3.406250   2607600   AMZN   \n",
       "1997-09-22  4.000000  4.677083  3.947917  4.500000  4.500000  16938000   AMZN   \n",
       "1997-09-29  4.145833  4.187500  3.958333  4.041667  4.041667   2371200   AMZN   \n",
       "1997-10-06  4.000000  4.125000  3.942708  4.125000  4.125000   2028000   AMZN   \n",
       "1997-10-17  3.614583  3.656250  3.520833  3.625000  3.625000   2534400   AMZN   \n",
       "1997-10-21  3.958333  4.437500  3.854167  4.427083  4.427083  12096000   AMZN   \n",
       "1997-10-31  5.364583  5.458333  4.968750  5.083333  5.083333   5026800   AMZN   \n",
       "1997-11-05  4.979167  5.119792  4.875000  4.875000  4.875000   3093600   AMZN   \n",
       "1997-11-07  4.416667  4.640625  4.385417  4.479167  4.479167   2626800   AMZN   \n",
       "1997-11-17  4.229167  4.541667  4.218750  4.375000  4.375000   7394400   AMZN   \n",
       "\n",
       "            adjclose_15  true_adjclose_15  buy_profit  sell_profit  \n",
       "1997-08-06    -8.748855          2.317708         0.0    -0.067708  \n",
       "1997-08-07    -8.921314          2.375000         0.0    -0.197917  \n",
       "1997-08-18    -9.788497          3.239583         0.0    -1.197916  \n",
       "1997-08-19    -9.870554          3.302083         0.0    -1.135416  \n",
       "1997-08-21   -10.025806          3.687500         0.0    -1.572917  \n",
       "1997-09-05   -10.105702          4.166667         0.0    -1.666667  \n",
       "1997-09-08    -9.969608          4.041667         0.0    -1.041667  \n",
       "1997-09-10    -9.441730          4.020833         0.0    -0.718750  \n",
       "1997-09-12    -9.037761          4.015625         0.0    -0.328125  \n",
       "1997-09-15    -8.836848          4.125000         0.0    -1.031250  \n",
       "1997-09-17    -8.462343          4.005208         0.0    -0.598958  \n",
       "1997-09-22    -7.622609          4.000000         0.0     0.500000  \n",
       "1997-09-29    -6.254580          3.822917         0.0     0.218750  \n",
       "1997-10-06    -6.093350          4.270833         0.0    -0.145833  \n",
       "1997-10-17    -7.022874          4.479167         0.0    -0.854167  \n",
       "1997-10-21    -7.048798          3.947917         0.0     0.479166  \n",
       "1997-10-31    -5.625153          4.489583         0.0     0.593750  \n",
       "1997-11-05    -5.376828          4.260417         0.0     0.614583  \n",
       "1997-11-07    -5.358926          4.270833         0.0     0.208334  \n",
       "1997-11-17    -5.870422          4.687500         0.0    -0.312500  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjclose_15</th>\n",
       "      <th>true_adjclose_15</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-29</th>\n",
       "      <td>3230.000000</td>\n",
       "      <td>3236.989990</td>\n",
       "      <td>3184.550049</td>\n",
       "      <td>3206.199951</td>\n",
       "      <td>3206.199951</td>\n",
       "      <td>4293600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3270.413574</td>\n",
       "      <td>3180.739990</td>\n",
       "      <td>-25.459961</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-17</th>\n",
       "      <td>3263.600098</td>\n",
       "      <td>3320.909912</td>\n",
       "      <td>3259.500000</td>\n",
       "      <td>3308.639893</td>\n",
       "      <td>3308.639893</td>\n",
       "      <td>3297500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3304.185059</td>\n",
       "      <td>3057.639893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02</th>\n",
       "      <td>3143.469971</td>\n",
       "      <td>3163.520020</td>\n",
       "      <td>3087.120117</td>\n",
       "      <td>3094.530029</td>\n",
       "      <td>3094.530029</td>\n",
       "      <td>2595800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3176.813965</td>\n",
       "      <td>3137.500000</td>\n",
       "      <td>42.969971</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-03</th>\n",
       "      <td>3081.179932</td>\n",
       "      <td>3107.780029</td>\n",
       "      <td>2995.000000</td>\n",
       "      <td>3005.000000</td>\n",
       "      <td>3005.000000</td>\n",
       "      <td>3988700</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3158.854980</td>\n",
       "      <td>3087.070068</td>\n",
       "      <td>82.070068</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-15</th>\n",
       "      <td>3074.570068</td>\n",
       "      <td>3082.239990</td>\n",
       "      <td>3032.090088</td>\n",
       "      <td>3081.679932</td>\n",
       "      <td>3081.679932</td>\n",
       "      <td>2913600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3080.858154</td>\n",
       "      <td>3223.820068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-142.140137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-23</th>\n",
       "      <td>3127.000000</td>\n",
       "      <td>3182.000000</td>\n",
       "      <td>3120.850098</td>\n",
       "      <td>3137.500000</td>\n",
       "      <td>3137.500000</td>\n",
       "      <td>3817300</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3105.613037</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-195.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>3044.060059</td>\n",
       "      <td>3056.659912</td>\n",
       "      <td>2996.000000</td>\n",
       "      <td>3052.030029</td>\n",
       "      <td>3052.030029</td>\n",
       "      <td>3312900</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3110.058594</td>\n",
       "      <td>3372.010010</td>\n",
       "      <td>319.979980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-06</th>\n",
       "      <td>3223.750000</td>\n",
       "      <td>3247.310059</td>\n",
       "      <td>3217.040039</td>\n",
       "      <td>3223.820068</td>\n",
       "      <td>3223.820068</td>\n",
       "      <td>2537800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3135.195801</td>\n",
       "      <td>3417.429932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-193.609863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-07</th>\n",
       "      <td>3233.800049</td>\n",
       "      <td>3303.610107</td>\n",
       "      <td>3223.649902</td>\n",
       "      <td>3279.389893</td>\n",
       "      <td>3279.389893</td>\n",
       "      <td>3346200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3158.339355</td>\n",
       "      <td>3458.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-179.110107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-16</th>\n",
       "      <td>3380.000000</td>\n",
       "      <td>3406.800049</td>\n",
       "      <td>3355.590088</td>\n",
       "      <td>3399.439941</td>\n",
       "      <td>3399.439941</td>\n",
       "      <td>3186000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3342.264404</td>\n",
       "      <td>3291.610107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.829834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-19</th>\n",
       "      <td>3390.330078</td>\n",
       "      <td>3435.929932</td>\n",
       "      <td>3360.159912</td>\n",
       "      <td>3372.010010</td>\n",
       "      <td>3372.010010</td>\n",
       "      <td>2725400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3356.140625</td>\n",
       "      <td>3190.489990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>181.520020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-20</th>\n",
       "      <td>3373.600098</td>\n",
       "      <td>3382.989990</td>\n",
       "      <td>3316.000000</td>\n",
       "      <td>3334.689941</td>\n",
       "      <td>3334.689941</td>\n",
       "      <td>2623000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3363.611328</td>\n",
       "      <td>3223.909912</td>\n",
       "      <td>-110.780029</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-21</th>\n",
       "      <td>3316.000000</td>\n",
       "      <td>3362.860107</td>\n",
       "      <td>3303.810059</td>\n",
       "      <td>3362.020020</td>\n",
       "      <td>3362.020020</td>\n",
       "      <td>2211200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3365.614014</td>\n",
       "      <td>3151.939941</td>\n",
       "      <td>-210.080078</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-26</th>\n",
       "      <td>3348.000000</td>\n",
       "      <td>3428.449951</td>\n",
       "      <td>3330.939941</td>\n",
       "      <td>3409.000000</td>\n",
       "      <td>3409.000000</td>\n",
       "      <td>4880700</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3362.019043</td>\n",
       "      <td>3270.389893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>138.610107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-27</th>\n",
       "      <td>3443.469971</td>\n",
       "      <td>3460.000000</td>\n",
       "      <td>3398.010010</td>\n",
       "      <td>3417.429932</td>\n",
       "      <td>3417.429932</td>\n",
       "      <td>3827100</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3368.244629</td>\n",
       "      <td>3232.280029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.149902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-03</th>\n",
       "      <td>3484.729980</td>\n",
       "      <td>3486.649902</td>\n",
       "      <td>3372.699951</td>\n",
       "      <td>3386.489990</td>\n",
       "      <td>3386.489990</td>\n",
       "      <td>5875500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3422.923096</td>\n",
       "      <td>3244.989990</td>\n",
       "      <td>-141.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10</th>\n",
       "      <td>3282.320068</td>\n",
       "      <td>3283.000000</td>\n",
       "      <td>3190.000000</td>\n",
       "      <td>3190.489990</td>\n",
       "      <td>3190.489990</td>\n",
       "      <td>5838600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3354.809814</td>\n",
       "      <td>3218.649902</td>\n",
       "      <td>28.159912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-21</th>\n",
       "      <td>3250.000000</td>\n",
       "      <td>3256.689941</td>\n",
       "      <td>3197.010010</td>\n",
       "      <td>3203.080078</td>\n",
       "      <td>3203.080078</td>\n",
       "      <td>4104900</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3242.391602</td>\n",
       "      <td>3383.870117</td>\n",
       "      <td>180.790039</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-26</th>\n",
       "      <td>3274.590088</td>\n",
       "      <td>3295.729980</td>\n",
       "      <td>3258.510010</td>\n",
       "      <td>3265.159912</td>\n",
       "      <td>3265.159912</td>\n",
       "      <td>2384000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3249.824951</td>\n",
       "      <td>3489.239990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-224.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-09</th>\n",
       "      <td>3272.870117</td>\n",
       "      <td>3297.580078</td>\n",
       "      <td>3270.699951</td>\n",
       "      <td>3281.149902</td>\n",
       "      <td>3281.149902</td>\n",
       "      <td>2455500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3240.192627</td>\n",
       "      <td>3440.159912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-159.010010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   open         high          low        close     adjclose  \\\n",
       "2021-01-29  3230.000000  3236.989990  3184.550049  3206.199951  3206.199951   \n",
       "2021-02-17  3263.600098  3320.909912  3259.500000  3308.639893  3308.639893   \n",
       "2021-03-02  3143.469971  3163.520020  3087.120117  3094.530029  3094.530029   \n",
       "2021-03-03  3081.179932  3107.780029  2995.000000  3005.000000  3005.000000   \n",
       "2021-03-15  3074.570068  3082.239990  3032.090088  3081.679932  3081.679932   \n",
       "2021-03-23  3127.000000  3182.000000  3120.850098  3137.500000  3137.500000   \n",
       "2021-03-26  3044.060059  3056.659912  2996.000000  3052.030029  3052.030029   \n",
       "2021-04-06  3223.750000  3247.310059  3217.040039  3223.820068  3223.820068   \n",
       "2021-04-07  3233.800049  3303.610107  3223.649902  3279.389893  3279.389893   \n",
       "2021-04-16  3380.000000  3406.800049  3355.590088  3399.439941  3399.439941   \n",
       "2021-04-19  3390.330078  3435.929932  3360.159912  3372.010010  3372.010010   \n",
       "2021-04-20  3373.600098  3382.989990  3316.000000  3334.689941  3334.689941   \n",
       "2021-04-21  3316.000000  3362.860107  3303.810059  3362.020020  3362.020020   \n",
       "2021-04-26  3348.000000  3428.449951  3330.939941  3409.000000  3409.000000   \n",
       "2021-04-27  3443.469971  3460.000000  3398.010010  3417.429932  3417.429932   \n",
       "2021-05-03  3484.729980  3486.649902  3372.699951  3386.489990  3386.489990   \n",
       "2021-05-10  3282.320068  3283.000000  3190.000000  3190.489990  3190.489990   \n",
       "2021-05-21  3250.000000  3256.689941  3197.010010  3203.080078  3203.080078   \n",
       "2021-05-26  3274.590088  3295.729980  3258.510010  3265.159912  3265.159912   \n",
       "2021-06-09  3272.870117  3297.580078  3270.699951  3281.149902  3281.149902   \n",
       "\n",
       "             volume ticker  adjclose_15  true_adjclose_15  buy_profit  \\\n",
       "2021-01-29  4293600   AMZN  3270.413574       3180.739990  -25.459961   \n",
       "2021-02-17  3297500   AMZN  3304.185059       3057.639893    0.000000   \n",
       "2021-03-02  2595800   AMZN  3176.813965       3137.500000   42.969971   \n",
       "2021-03-03  3988700   AMZN  3158.854980       3087.070068   82.070068   \n",
       "2021-03-15  2913600   AMZN  3080.858154       3223.820068    0.000000   \n",
       "2021-03-23  3817300   AMZN  3105.613037       3333.000000    0.000000   \n",
       "2021-03-26  3312900   AMZN  3110.058594       3372.010010  319.979980   \n",
       "2021-04-06  2537800   AMZN  3135.195801       3417.429932    0.000000   \n",
       "2021-04-07  3346200   AMZN  3158.339355       3458.500000    0.000000   \n",
       "2021-04-16  3186000   AMZN  3342.264404       3291.610107    0.000000   \n",
       "2021-04-19  2725400   AMZN  3356.140625       3190.489990    0.000000   \n",
       "2021-04-20  2623000   AMZN  3363.611328       3223.909912 -110.780029   \n",
       "2021-04-21  2211200   AMZN  3365.614014       3151.939941 -210.080078   \n",
       "2021-04-26  4880700   AMZN  3362.019043       3270.389893    0.000000   \n",
       "2021-04-27  3827100   AMZN  3368.244629       3232.280029    0.000000   \n",
       "2021-05-03  5875500   AMZN  3422.923096       3244.989990 -141.500000   \n",
       "2021-05-10  5838600   AMZN  3354.809814       3218.649902   28.159912   \n",
       "2021-05-21  4104900   AMZN  3242.391602       3383.870117  180.790039   \n",
       "2021-05-26  2384000   AMZN  3249.824951       3489.239990    0.000000   \n",
       "2021-06-09  2455500   AMZN  3240.192627       3440.159912    0.000000   \n",
       "\n",
       "            sell_profit  \n",
       "2021-01-29     0.000000  \n",
       "2021-02-17   251.000000  \n",
       "2021-03-02     0.000000  \n",
       "2021-03-03     0.000000  \n",
       "2021-03-15  -142.140137  \n",
       "2021-03-23  -195.500000  \n",
       "2021-03-26     0.000000  \n",
       "2021-04-06  -193.609863  \n",
       "2021-04-07  -179.110107  \n",
       "2021-04-16   107.829834  \n",
       "2021-04-19   181.520020  \n",
       "2021-04-20     0.000000  \n",
       "2021-04-21     0.000000  \n",
       "2021-04-26   138.610107  \n",
       "2021-04-27   185.149902  \n",
       "2021-05-03     0.000000  \n",
       "2021-05-10     0.000000  \n",
       "2021-05-21     0.000000  \n",
       "2021-05-26  -224.080078  \n",
       "2021-06-09  -159.010010  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
